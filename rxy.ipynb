{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scapy.all import *\n",
    "import dpkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scapy.layers' has no attribute 'inet'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mscapy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mlayers\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m IP \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39minet\u001B[38;5;241m.\u001B[39mIP\n\u001B[0;32m      5\u001B[0m GET_THRESH \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m300\u001B[39m \u001B[38;5;66;03m# bytes\u001B[39;00m\n\u001B[0;32m      6\u001B[0m DOWN_THRESH \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m300\u001B[39m  \u001B[38;5;66;03m# bytes\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'scapy.layers' has no attribute 'inet'"
     ]
    }
   ],
   "source": [
    "\n",
    "from math import *\n",
    "import scapy.layers as layers\n",
    "IP = layers.inet.IP\n",
    "\n",
    "GET_THRESH = 300 # bytes\n",
    "DOWN_THRESH = 300  # bytes\n",
    "VIDEO_CHUNK_GETSIZE = 700 # bytes\n",
    "AUDIO_CHUNK_GETSIZE = 600 # bytes\n",
    "NETINFO_NUM = 25\n",
    "\n",
    "class Chunk():\n",
    "    def __init__(self, start_time = 0, server_ip='',ttfb = 0, download_time = 0, slack_time = 0, get_size=0, chunk_size = 0, type=\"\", protocol=\"\"):\n",
    "        self.start_time = start_time\n",
    "        self.server_ip = server_ip\n",
    "        self.ttfb = ttfb\n",
    "        self.download_time = download_time\n",
    "        self.slack_time = slack_time\n",
    "        self.get_size = get_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.type = type\n",
    "        self.protocol = protocol\n",
    "    def __str__(self):\n",
    "        return f\"{self.start_time} {self.ttfb} {self.download_time} {self.slack_time} {self.get_size} {self.chunk_size} {self.type} {self.protocol}\"\n",
    "    def __repr__(self):\n",
    "        return f\"{self.start_time} {self.ttfb} {self.download_time} {self.slack_time} {self.get_size} {self.chunk_size} {self.type} {self.protocol}\"\n",
    "\n",
    "\n",
    "def isUplink(p):\n",
    "    # IP = scapy.layers.inet.IP\n",
    "    return p[IP].src.startswith('192.168.')\n",
    "\n",
    "def detectAV(c):\n",
    "    if abs(c.get_size-VIDEO_CHUNK_GETSIZE) > abs(c.get_size-AUDIO_CHUNK_GETSIZE):\n",
    "        flag=0\n",
    "    else:\n",
    "        flag=1\n",
    "    if c.chunk_size<=80*1024:\n",
    "        flag=2\n",
    "    return flag\n",
    "\n",
    "def chunkDetection(filename):\n",
    "    a = rdpcap(filename)\n",
    "    meta_time = float(a[0].time)\n",
    "    chunk = {}\n",
    "    chunks = []\n",
    "    downFlag = {}\n",
    "    # IP = scapy.layers.inet.IP\n",
    "    for p in a:\n",
    "        if p.haslayer(IP):\n",
    "            ipSrc=p[IP].src\n",
    "            ipDst=p[IP].dst\n",
    "            pLen=p[IP].len\n",
    "            pHdr=p[IP].ihl*4\n",
    "            ip_time=float(p.time)\n",
    "            if isUplink(p) and pLen-pHdr > GET_THRESH:\n",
    "                if ipDst in chunk:\n",
    "                    chunk[ipDst].slack_time = ip_time - chunk[ipDst].download_time\n",
    "                    avFlag=detectAV(chunk[ipDst])\n",
    "                    if avFlag==0:\n",
    "                        # chunk[ipDst].type='a'\n",
    "                        chunks.append(chunk[ipDst])\n",
    "                    elif avFlag==1:\n",
    "                        # chunk[ipDst].type='v'\n",
    "                        chunks.append(chunk[ipDst])\n",
    "                    else:\n",
    "                        chunk.pop(ipDst)\n",
    "                        downFlag.pop(ipDst)\n",
    "                chunk[ipDst] = Chunk(start_time=ip_time, get_size=pLen-pHdr, server_ip=ipDst)\n",
    "                downFlag[ipDst] = False\n",
    "            elif not isUplink(p) and pLen > DOWN_THRESH:\n",
    "                if ipSrc in chunk:\n",
    "                    if not downFlag[ipSrc]:\n",
    "                        chunk[ipSrc].ttfb = ip_time\n",
    "                        downFlag[ipSrc] = True\n",
    "                    chunk[ipSrc].download_time = ip_time\n",
    "                    chunk[ipSrc].chunk_size += pLen - pHdr\n",
    "                    chunk[ipSrc].protocol = p.proto\n",
    "    \n",
    "    for c in chunk.values():\n",
    "        avFlag=detectAV(c)\n",
    "        if avFlag==0:\n",
    "            # c.type='a'\n",
    "            chunks.append(c)\n",
    "        elif avFlag==1:\n",
    "            # c.type='v'\n",
    "            chunks.append(c)\n",
    "    return meta_time, chunks,chunk\n",
    "\n",
    "filename = 'RequetDataSetNew/A0/PCAP_FILES/baseline_Jan17_exp_31.pcap'\n",
    "meta_time, chunks, chunk=chunkDetection(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks))\n",
    "print(len(chunk))\n",
    "print(len(chunk.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 阮阮Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pcap file...\n",
      "Parsing pcap file...\n",
      "Calcilate chunk...\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "import dpkt\n",
    "import numpy as np\n",
    "import socket\n",
    "np.set_printoptions(suppress=True,   precision=20,  threshold=10,  linewidth=40)\n",
    "GET_THRESH = 300  # bytes\n",
    "DOWN_THRESH = 300  # bytes\n",
    "VIDEO_CHUNK_GETSIZE = 700  # bytes\n",
    "AUDIO_CHUNK_GETSIZE = 600  # bytes\n",
    "\n",
    "class Chunk():\n",
    "    def __init__(self, GetTimestamp=0, GetSize=0, DownStart=0, DownEnd=0, DownSize=0, type=0, GetProtocol=\"\", serverIP = \"\"):\n",
    "        self.GetTimestamp = GetTimestamp  #发送请求的时间戳\n",
    "        self.GetSize = GetSize  #Get请求的长度\n",
    "        self.DownStart = DownStart  #块的第一个下行包的时间戳\n",
    "        self.DownEnd = DownEnd  #块的最后一个下行包的时间戳\n",
    "        self.DownSize = DownSize  #块的所有下行包的大小之和\n",
    "        self.type = type  #类型：视频或音频\n",
    "        self.GetProtocol = GetProtocol  #协议\n",
    "        self.serverIP = serverIP\n",
    "    def getGetTimestamp(self):\n",
    "        return self.GetTimestamp\n",
    "\n",
    "    def getDownEnd(self):\n",
    "        return  self.DownEnd\n",
    "\n",
    "    def getServerIP(self):\n",
    "        return self.serverIP\n",
    "\n",
    "    def getGetSize(self):\n",
    "        return self.GetSize\n",
    "\n",
    "    def setType(self, newType):\n",
    "        self.type = newType\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        self_small = True\n",
    "        A = self.GetTimestamp\n",
    "        B = other.GetTimestamp\n",
    "        self_small = A < B\n",
    "        return self_small\n",
    "\n",
    "    def detectAV(self):\n",
    "        # 论文里是根据GetSize, DownSize, GetProtocol来区分视频块，音频块和后台流量的\n",
    "        flag = 0\n",
    "        if self.DownSize <= 80*1024:\n",
    "            flag = 2\n",
    "        else:\n",
    "            if abs(self.GetSize-VIDEO_CHUNK_GETSIZE) > abs(self.GetSize-AUDIO_CHUNK_GETSIZE): #判断这个大小更接近视频块还是音频块\n",
    "                flag = 0\n",
    "            else:\n",
    "                flag = 1\n",
    "        # 基于协议号的筛选\n",
    "        return flag\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.serverIP} {self.GetTimestamp} {self.GetSize} {self.DownStart} {self.DownEnd} {self.DownSize} {self.type} {self.GetProtocol}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.serverIP} {self.GetTimestamp} {self.GetSize} {self.DownStart} {self.DownEnd} {self.DownSize} {self.type} {self.GetProtocol}\"\n",
    "\n",
    "def inet_to_str(inet):\n",
    "    try:\n",
    "        return socket.inet_ntop(socket.AF_INET, inet)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def isUplink(src):\n",
    "    return src[0:7] == '192.168'\n",
    "\n",
    "\n",
    "def ChunkDetection(filename):\n",
    "    #解析pcap文件\n",
    "    print(\"Reading pcap file...\")\n",
    "\n",
    "    #读入pcap文件\n",
    "    f = open(filename, 'rb')\n",
    "    pcap = dpkt.pcap.Reader(f)\n",
    "\n",
    "    # meta_time = 0\n",
    "    ii = 0\n",
    "    #pcap是Reader类，无法切片，目前先这样写\n",
    "    # for ts, buf in pcap:\n",
    "    #     ii += 1\n",
    "    #     if ii == 1:\n",
    "    #         meta_time = ts\n",
    "    #         break\n",
    "    # end_time = meta_time\n",
    "    chunks = {}\n",
    "    chunksValue = []\n",
    "    downFlag = {}  #用于标识是不是第一个下行报文\n",
    "\n",
    "    print(\"Parsing pcap file...\")\n",
    "\n",
    "    times = []\n",
    "    ip_srcs = []\n",
    "    ip_dsts = []\n",
    "    ip_lens = []\n",
    "    ip_ihls = []\n",
    "    ip_protos = []\n",
    "    for ts, buf in pcap:\n",
    "        # 这里也是对没有IP段的包过滤掉\n",
    "        eth = dpkt.ethernet.Ethernet(buf)\n",
    "        if eth.type != dpkt.ethernet.ETH_TYPE_IP:\n",
    "            continue\n",
    "        # ts是时间戳\n",
    "        times.append(ts)\n",
    "        ip = eth.data\n",
    "        ip_src = inet_to_str(ip.src)  #\n",
    "        ip_dst = inet_to_str(ip.dst)  #\n",
    "        ip_srcs.append(ip_src)  # src\n",
    "        ip_dsts.append(ip_dst)  # dst\n",
    "        ip_lens.append(ip.len)  # len\n",
    "        ip_protos.append(ip.p)  # proto:协议号tcp是6\n",
    "        ip_ihls.append(ip.hl)  # ihl\n",
    "    first_end_time = (times[0],times[-1])\n",
    "    print(\"Calcilate chunk...\")\n",
    "    sumGetSize = 0\n",
    "    for n in range(len(ip_srcs)):\n",
    "        ipSrc = ip_srcs[n]\n",
    "        ipDst = ip_dsts[n]\n",
    "        ipSize = ip_lens[n] - ip_ihls[n] * 4\n",
    "        ipTime = times[n]\n",
    "        ipProto = ip_protos[n]\n",
    "        #end_time = ipTime\n",
    "        if isUplink(ipSrc) and ipSize > GET_THRESH:\n",
    "            # 上行包，2种情况：（1）已经出现过的，则处理上一个发往这个站点的块，并初始化新块；（2）没有出现过，初始化新块\n",
    "            if ipDst in chunks:  #意味着以这一ip地址为目的地址的块已经结束，以这一ip地址为目的地址的块马上开始\n",
    "                #筛选：刚刚结束的块是否是音频或视频块\n",
    "                avFlag = chunks[ipDst].detectAV()\n",
    "                if not avFlag == 2:  # 音频块\n",
    "                    sumGetSize += chunks[ipDst].getGetSize()\n",
    "                    chunksValue.append(chunks[ipDst])\n",
    "                else:  # 后台流量\n",
    "                    chunks.pop(ipDst)  # 抛弃这个块\n",
    "                    downFlag.pop(ipDst)\n",
    "            #初始化新块\n",
    "            chunks[ipDst] = Chunk(GetTimestamp=ipTime, GetSize=ipSize, GetProtocol=ipProto, serverIP = ipDst)\n",
    "            downFlag[ipDst] = False\n",
    "        elif not isUplink(ipSrc) and ipSize > DOWN_THRESH:\n",
    "            if ipSrc in chunks:\n",
    "            # 下行包，2种情况：（1）是Get请求的第一个下行包，记录时间，更新大小和时间；（2）不是Get请求的第一个下行包，更新大小和时间\n",
    "                if not downFlag[ipSrc]:\n",
    "                    chunks[ipSrc].DownStart = ipTime  # 收到第一个下行包的时间\n",
    "                    downFlag[ipSrc] = True\n",
    "                chunks[ipSrc].DownEnd = ipTime\n",
    "                chunks[ipSrc].DownSize += ipSize\n",
    "                chunks[ipSrc].protocol = ipProto\n",
    "\n",
    "    for c in chunks.values():\n",
    "        avFlag = c.detectAV()\n",
    "        if not avFlag == 2:  # 音频块\n",
    "            sumGetSize += c.getGetSize()\n",
    "            chunksValue.append(c)\n",
    "\n",
    "    # 区分音频块和视频块\n",
    "    chunkNum = len(chunksValue)\n",
    "    ave_GetSize = sumGetSize / chunkNum\n",
    "    for s_chunk in chunksValue:\n",
    "        if (s_chunk.getGetSize() > ave_GetSize):\n",
    "            s_chunk.setType(0)\n",
    "        else:\n",
    "            s_chunk.setType(1)\n",
    "\n",
    "    return chunkNum, chunksValue,first_end_time\n",
    "\n",
    "def getChunkMetrics(chunkNum, chunksValue):\n",
    "    sortChunks = sorted(chunksValue)\n",
    "    #顺序：'start_time', 'type', 'ttfb', 'download_time', 'end_time', 'get_size', 'chunk_size'\n",
    "    output_data = np.zeros((chunkNum, 7))\n",
    "    i = 0\n",
    "    for c in sortChunks:\n",
    "        start_time = c.GetTimestamp\n",
    "        ttfb = c.DownStart - start_time\n",
    "        download_time = c.DownEnd - c.DownStart\n",
    "        end_time = c.DownEnd\n",
    "        get_size = c.GetSize\n",
    "        chunk_size = c.DownSize\n",
    "        type = c.type\n",
    "        output_data[i] = np.array([start_time, type, ttfb, download_time, end_time, get_size, chunk_size])\n",
    "        i += 1\n",
    "    print(i)\n",
    "    return output_data\n",
    "\n",
    "filename = 'RequetDataSetNew/A0/PCAP_FILES/baseline_Jan17_exp_31.pcap'\n",
    "# 1. chunkDetection\n",
    "chunkNum, chunksValue0,fe_time = ChunkDetection(filename)\n",
    "# 2. chunkMetrics\n",
    "output = getChunkMetrics(chunkNum, chunksValue0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def getFeature(epoch_msec,cm,first_end_time):\n",
    "    t = epoch_msec / 1000\n",
    "    if t < first_end_time[0] or t > first_end_time[1]:\n",
    "        return None\n",
    "    s = []\n",
    "    start_time = 0\n",
    "    type = 1\n",
    "    ttfb = 2\n",
    "    download_time = 3\n",
    "    end_time = 4\n",
    "    get_size = 5\n",
    "    chunk_size = 6\n",
    "    flag = True\n",
    "    last_chunk=[]\n",
    "    for w in range(1, 21):\n",
    "        period = w * 10.0\n",
    "        if t - period < first_end_time[0]:\n",
    "            total_number_of_chunks_v = -1\n",
    "            avg_chunk_size_v = -1\n",
    "            download_time_v = -1\n",
    "            total_number_of_chunks_a = -1\n",
    "            avg_chunk_size_a = -1\n",
    "            download_time_a = -1\n",
    "        else:\n",
    "            is_out_border = (cm[:,end_time] > t - period) & (cm[:,start_time] < t)\n",
    "            if flag and len(cm[is_out_border]) != 0:\n",
    "                last_chunk=cm[is_out_border]\n",
    "                flag = False\n",
    "            total_number_of_chunks_v = cm[(cm[:,type] == 0) & is_out_border].shape[0]\n",
    "            if total_number_of_chunks_v == 0:\n",
    "                avg_chunk_size_v = 0\n",
    "            else:\n",
    "                avg_chunk_size_v = cm[:,chunk_size][(cm[:,type] == 0) & (cm[:,end_time] > t - period) & (cm[:,start_time] < t)].mean()\n",
    "            download_time_v = cm[:,download_time][(cm[:,type] == 0) & is_out_border].sum()\n",
    "\n",
    "\n",
    "            total_number_of_chunks_a = cm[(cm[:,type] == 1) & is_out_border].shape[0]\n",
    "            if total_number_of_chunks_a == 0:\n",
    "                avg_chunk_size_a = 0\n",
    "            else:\n",
    "                avg_chunk_size_a = cm[:,chunk_size][(cm[:,type] == 1) & (cm[:,end_time] > t - period) & (cm[:,start_time] < t)].mean()\n",
    "            download_time_a = cm[:,download_time][(cm[:,type] == 1) & is_out_border].sum()\n",
    "        s += [total_number_of_chunks_v, avg_chunk_size_v, download_time_v,\n",
    "              total_number_of_chunks_a, avg_chunk_size_a, download_time_a]\n",
    "    # TODO\n",
    "    # s += [离t最近的chunk，chunk的feature]\n",
    "\n",
    "    s += list(last_chunk[-1])\n",
    "    # s += list(last_chunk[0][-1])\n",
    "    return s\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pcap file...\n",
      "Parsing pcap file...\n",
      "Calcilate chunk...\n",
      "56\n"
     ]
    },
    {
     "data": {
      "text/plain": "[2,\n 1279861.0,\n 2.74914288520813,\n 0,\n 0,\n 0.0,\n 2,\n 1279861.0,\n 2.74914288520813,\n 0,\n 0,\n 0.0,\n 2,\n 1279861.0,\n 2.74914288520813,\n 0,\n 0,\n 0.0,\n 3,\n 1492881.6666666667,\n 4.736539840698242,\n 0,\n 0,\n 0.0,\n 3,\n 1492881.6666666667,\n 4.736539840698242,\n 0,\n 0,\n 0.0,\n 5,\n 1412717.6,\n 8.965434789657593,\n 0,\n 0,\n 0.0,\n 5,\n 1412717.6,\n 8.965434789657593,\n 0,\n 0,\n 0.0,\n 6,\n 1524897.5,\n 10.465993881225586,\n 0,\n 0,\n 0.0,\n 7,\n 1564473.857142857,\n 12.488471984863281,\n 0,\n 0,\n 0.0,\n 7,\n 1564473.857142857,\n 12.488471984863281,\n 0,\n 0,\n 0.0,\n 8,\n 1445585.0,\n 13.299987077713013,\n 0,\n 0,\n 0.0,\n 9,\n 1495504.4444444445,\n 15.083724021911621,\n 0,\n 0,\n 0.0,\n 9,\n 1495504.4444444445,\n 15.083724021911621,\n 0,\n 0,\n 0.0,\n 11,\n 1420095.0,\n 17.112349271774292,\n 0,\n 0,\n 0.0,\n 11,\n 1420095.0,\n 17.112349271774292,\n 0,\n 0,\n 0.0,\n 11,\n 1420095.0,\n 17.112349271774292,\n 0,\n 0,\n 0.0,\n 12,\n 1464953.25,\n 19.117595434188843,\n 0,\n 0,\n 0.0,\n 13,\n 1400798.6923076923,\n 19.849315404891968,\n 0,\n 0,\n 0.0,\n 14,\n 1448151.642857143,\n 21.706393480300903,\n 0,\n 0,\n 0.0,\n 16,\n 1430921.875,\n 24.502686738967896,\n 0,\n 0,\n 0.0,\n 1516209854.791172,\n 0.0,\n 0.01102304458618164,\n 1.036834955215454,\n 1516209855.83903,\n 1260.0,\n 616946.0]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'RequetDataSetNew/A0/PCAP_FILES/baseline_Jan17_exp_31.pcap'\n",
    "# 1. chunkDetection\n",
    "chunkNum, chunksValue0,fe_time = ChunkDetection(filename)\n",
    "# 2. chunkMetrics\n",
    "output = getChunkMetrics(chunkNum, chunksValue0)\n",
    "\n",
    "getFeature(1516209864093,output,fe_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: LabelDataSet/A0\\baseline_Jan17_exp_31_merged_label.csv\n",
      "baseline_Jan17_exp_31\n",
      "Reading pcap file...\n",
      "Parsing pcap file...\n",
      "Calcilate chunk...\n",
      "56\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "ans = []\n",
    "# for dataset in dataset_list:\n",
    "dataset_folder = 'LabelDataSet/A0/'\n",
    "files = glob.glob(dataset_folder + '*_label.csv')\n",
    "# cnt=0\n",
    "for i in range(1):\n",
    "    file_name = files[i]\n",
    "    print(\"Processing file: \" + file_name)\n",
    "    # if (i >= len_list[dataset]):\n",
    "    # \tbreak\n",
    "    file = pd.read_csv(file_name)  # wxh's csv name\n",
    "    print(os.path.split(file_name)[1].split('/')[-1][:-17])\n",
    "    feature_file = 'RequetDataSetNew/' +'A0'+'/PCAP_FILES/' + os.path.split(file_name)[1].split('/')[-1][:-17] + '.pcap'  # yhy's pcap name\n",
    "    # 1. chunkDetection\n",
    "    chunkNum, chunksValue0,fe_time = ChunkDetection(filename)\n",
    "    # 2. chunkMetrics\n",
    "    output = getChunkMetrics(chunkNum, chunksValue0)\n",
    "    lable_size = file.shape[0]\n",
    "    for i in range(0, lable_size, 50):\n",
    "        feature = getFeature(file.iloc[i, 0],output,fe_time)\n",
    "        if not feature:\n",
    "            continue\n",
    "        feature.append(file_name + \"-\" + str(file.iloc[i, 0]))\n",
    "        feature.extend(file.iloc[i, 1:])\n",
    "        ans.append(feature)\n",
    "        print(feature[121])\n",
    "# out_file = open('test_data/test_data111.csv', 'w', newline='')  # output   be careful of filename\n",
    "# writer = csv.writer(out_file)\n",
    "# keys = ['label' + str(i) for i in range(120)]\n",
    "# keys.extend(['','','','','','',''])\n",
    "# keys.extend(['filename_time', 'status', 'BuffWarning', 'Resolution'])\n",
    "# writer.writerow(keys)\n",
    "# for i in ans:\n",
    "#     writer.writerow(i)\n",
    "# out_file.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: LabelDataSet/A0\\baseline_Jan17_exp_31_merged_label.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "ans = []\n",
    "# for dataset in dataset_list:\n",
    "dataset_folder = 'LabelDataSet/A0/'\n",
    "files = glob.glob(dataset_folder + '*_label.csv')\n",
    "# cnt=0\n",
    "for i in range(1):\n",
    "    file_name = files[i]\n",
    "    print(\"Processing file: \" + file_name)\n",
    "    # if (i >= len_list[dataset]):\n",
    "    # \tbreak\n",
    "    file = pd.read_csv(file_name).to_numpy()  # wxh's csv name\n",
    "\n",
    "    # print(os.path.split(file_name)[1].split('/')[-1][:-17])\n",
    "    # feature_file = 'RequetDataSetNew/' +'A0'+'/PCAP_FILES/' + os.path.split(file_name)[1].split('/')[-1][:-17] + '.pcap'  # yhy's pcap name\n",
    "    # # 1. chunkDetection\n",
    "    # chunkNum, chunksValue0,fe_time = ChunkDetection(filename)\n",
    "    # # 2. chunkMetrics\n",
    "    # output = getChunkMetrics(chunkNum, chunksValue0)\n",
    "    # lable_size = file.shape[0]\n",
    "    # for i in range(0, lable_size, 50):\n",
    "    #     feature = getFeature(file.iloc[i, 0],output,fe_time)\n",
    "    #     if not feature:\n",
    "    #         continue\n",
    "    #     feature.append(file_name + \"-\" + str(file.iloc[i, 0]))\n",
    "    #     feature.extend(file.iloc[i, 1:])\n",
    "    #     ans.append(feature)\n",
    "    #     print(feature[121])\n",
    "# out_file = open('test_data/test_data111.csv', 'w', newline='')  # output   be careful of filename\n",
    "# writer = csv.writer(out_file)\n",
    "# keys = ['label' + str(i) for i in range(120)]\n",
    "# keys.extend(['','','','','','',''])\n",
    "# keys.extend(['filename_time', 'status', 'BuffWarning', 'Resolution'])\n",
    "# writer.writerow(keys)\n",
    "# for i in ans:\n",
    "#     writer.writerow(i)\n",
    "# out_file.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(5768, 4)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<_csv.reader at 0x24971ddfe80>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "servers = {sortChunks[0].getServerIP()}\n",
    "for c in sortChunks:\n",
    "    sIP = c.getServerIP()\n",
    "    print(c.getServerIP())\n",
    "    servers.add(sIP)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sortChunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [9], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m fristTime \u001B[38;5;241m=\u001B[39m sortChunks[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mgetGetTimestamp()\n\u001B[0;32m      3\u001B[0m lastTime \u001B[38;5;241m=\u001B[39m sortChunks[\u001B[38;5;28mlen\u001B[39m(sortChunks)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mgetDownEnd()\n\u001B[0;32m      5\u001B[0m timeList1 \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sortChunks' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fristTime = sortChunks[0].getGetTimestamp()\n",
    "lastTime = sortChunks[len(sortChunks)-1].getDownEnd()\n",
    "\n",
    "timeList1 = []\n",
    "timeList2 = []\n",
    "for c in sortChunks:\n",
    "    if (c.getServerIP() == '173.194.7.39'):\n",
    "        timeList1.append((c.getGetTimestamp()-fristTime, c.getDownEnd()-c.getGetTimestamp()))\n",
    "    if (c.getServerIP() == '23.206.171.9'):\n",
    "        timeList2.append((c.getGetTimestamp()-fristTime, c.getDownEnd()-c.getGetTimestamp()))\n",
    "\n",
    "print(timeList2)\n",
    "plt.figure(1,figsize=(20,4))\n",
    "plt.broken_barh(timeList1, (0,1), facecolors ='cyan')\n",
    "plt.broken_barh(timeList2, (2,1), facecolors ='green')\n",
    "plt.xlim(0, 500)\n",
    "#plt.xticks(np.arange(0, 500, 20))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "dip2022",
   "language": "python",
   "display_name": "dip2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
